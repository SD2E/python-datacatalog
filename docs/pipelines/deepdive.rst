===============================
ManagedPipelineJob: A Deep Dive
===============================

This document will demonstrate the specifics of parameterizing a
``ManagedPipelineJob``. The behaviors documented below also apply to
``ReactorManagedPipelineJob``, as it is a subclass of ``ManagedPipelineJob``.

Conceptual Overview
-------------------

The essential function of the ``ManagedPipelineJob`` class is to link the Data
Catalog representation of a **PipelineJob** with its **Pipeline**, inputs,
references, and experimental metadata to facilitate metadata-driven discovery.

The foundation of this linkage between data and compute is the *Archive*
functionality in the Agave Jobs API. Briefly, an Agave's job's *archive path* is
a combination of Agave storage system and absolute path where the job's
outputs are copied to after successful execution of job code. We extend this
by minting a distinct archive path for each **PipelineJob** based on our
knowledge of the job's attributes. This lets us know that any files found under
said path were generated by that **PipelineJob**.

When a **PipelineJob** is created, its parameters are inspected to establish
which pipeline generated it and whether it refers to any managed  **file** or
**reference** entities. These connections provide additional context to the
job. To connect the nascent metadata subgraph to the rest of the metadata
graph, we linik the job to one or more **measurements** using a ``child_of``
linkage.

This may be slightly counter-intuitive at first. In the experimental metadata
schema, uploaded data **files** are children of **measurements**. The
PipelineJob linkage scheme transitively connects **files** generated by a
specific job to the **measurements** section of the metadata graph. The outcome
is that these two relationship graphs are operationally equivalent with
respect to file discovery:

* ``sample <-[child_of]- measurement <-[child_of]- upload``
* ``sample <-[child_of]- measurement <-[child_of]- pipelinejob <-[child_of]- product``

There is a powerful advantage to the latter relationship: **product files**
inherit metatdata properties from **measurements** but also (via
**pipelinejob**) the properties of **pipeline** and any **upload files** or **references**
associated with the job. This branched inheritance makes it easy to build complex
queries like *"fetch all CSV and XLSX files from samples collected at 30 degrees C
resulting from VALIDATED jobs generated by version 7.1 of the Flow Pipeline with
cytometer configuration ABC"*. It also helps ensure that such queries remain
performant, reproducible, and reliable.

Setup
-----

To make it easier to reference them later, the various value permutations for
all usage examples are defined immediately below. Generic boilerplate for
setting up similar tests for one's own interactive use is also included.

.. code-block:: pycon

   >>> from settings import settings
   >>> from datacatalog.managers.pipelinejobs import ManagedPipelineJob as Job
   >>> experiments = ['experiment.tacc.10001']
   >>> samples = ['sample.tacc.20001']
   >>> measurements1 = ['measurement.tacc.0xDEADBEEF', 'measurement.tacc.0xDEADBEF0']
   >>> measurements2 = ['10483e8d-6602-532a-8941-176ce20dd05a', 'measurement.tacc.0xDEADBEF0']
   >>> measurements3 = ['measurement.tacc.0xDEADBEF1']
   >>> data_w_inputs = {'alpha': 0.5, 'inputs': ['agave://data-sd2e-community/uploads/tacc/example/345.txt'], 'parameters': {'ref1': 'agave://data-sd2e-community/reference/novel_chassis/uma_refs/MG1655_WT/MG1655_WT.fa'}}

.. note: Load above-referenced experiments, measurements, etc. into a local
   test database using ``make bootstrap-tests``

.. note: ``settings`` comes from a file `settings.py`` created in the main
   python-datacatalog directory. An example is included with the repository
   that will point to a local Docker-based MongoDb. The value for
   ``pipelines.pipeline_uuid`` must be ``106c46ff-8186-5756-a934-071f4497b58d``
   for these specific examples to work.

UUID, Parentage, and Archive Path
---------------------------------

A ``ManagedPipelineJob`` is configured by passing keyword arguments at
instantiation: i.e.) ``ManagedPipelineJob(mongodb, pipelines, <param1=value1>...)``.
The combination and value of these arguments establish the UUID, parental
linkage, and archive path for the **PipelineJob**. This is entirely
**deterministic**, which will be demonstrated below. For more details on what
arguments are available (and how to use them), please consult the
**ManagedPipelineJob API** documentation.

Only Measurement
################

This is the default, simplest configuration. Pass one or more values for
``measurement_id``:

* Job UUID is based on pipeline, the measurements, and an empty ``data``
* Job is a child of the specified measurement(s)
* Job archive path reflects the combination of measurement(s)

.. code-block:: pycon

   >>> mpj = Job(settings.mongodb, settings.pipelines, measurement_id=measurements1)
   >>> mpj.setup()
   uuid : 107ece67-9a94-57a1-bcb5-59b71de4fb13
   pipeline_uuid : 106c46ff-8186-5756-a934-071f4497b58d
   data : {}
   child_of : ['104dae4d-a677-5991-ae1c-696d2ee9884e', '10483e8d-6602-532a-8941-176ce20dd05a']
   generated_by : ['106c46ff-8186-5756-a934-071f4497b58d']
   acted_on : []
   acted_using : []
   archive_uri: agave://data-sd2e-community/products/v2/106c46ff81865756a934071f4497b58d/lAdLj59APx0e6E4gD29V6AND/PAVpwrObxp5YjYRvrJOd5yVp

Note the contents of ``child_of`` - the two UUIDs reference the specified
measurement_id values, and also note this component of the archive path
``lAdLj59APx0e6E4gD29V6AND``, which is unique to the specified measurements.

Only Sample
###########

Now, let's try to change the archive path so that jobs that work on multiple
measurements all write to the same physical location. This can be helpful for
setting up secondary pipelines that perform aggregation.

Pass in just a value for the **sample** that is parent to the measurements from
above as ``sample_id``. Behind the scenes, a little mapping is done. The
sample_id is expanded to the list of measurements that comprise it. Thus:

* Job UUID is based on pipeline, the sample's measurements, and an empty ``data``
* Job is a child of the the sample's measurements,
* Job archive path reflects specific **sample_id**

.. code-block:: pycon

   >>> mpk = Job(settings.mongodb, settings.pipelines, sample_id=samples)
   >>> mpk.setup()
   uuid : 107786da-33b4-5e37-8596-350211cd06dc
   pipeline_uuid : 106c46ff-8186-5756-a934-071f4497b58d
   data : {}
   child_of : ['104dae4d-a677-5991-ae1c-696d2ee9884e', '10483e8d-6602-532a-8941-176ce20dd05a', '1041ab3f-5221-5c79-8781-8838dfb6eef9']
   generated_by : ['106c46ff-8186-5756-a934-071f4497b58d']
   acted_on : []
   acted_using : []
   archive_uri: agave://data-sd2e-community/products/v2/106c46ff81865756a934071f4497b58d/kZgygQV2EDAAkDLRzrep1gO2/PAVpwrObxp5YjYRvrJOd5yVp

The contents of ``child_of`` are different: There are **THREE** measurements
because the sample is actually parent to a third measurement not included in
the original set of ``measurements`` from the first demonstration! Furthermore,
since the metadata linkage is different, the job UUID differs as well.

Measurement and Sample
######################

The key to sending output from multiple measurements to a single archive path
is to pass measurements and also the parent sample. In this case:

* Job UUID is based on pipeline, the measurements, and an empty ``data``
* Job is a child of the specified measurement(s)
* Job archive path reflects specific **sample_id**

.. code-block:: pycon

   >>> mpl = Job(settings.mongodb, settings.pipelines, sample_id=samples, measurement_id=measurements1)
   >>> mpl.setup()
   uuid : 107ece67-9a94-57a1-bcb5-59b71de4fb13
   pipeline_uuid : 106c46ff-8186-5756-a934-071f4497b58d
   data : {}
   child_of : ['104dae4d-a677-5991-ae1c-696d2ee9884e', '10483e8d-6602-532a-8941-176ce20dd05a']
   generated_by : ['106c46ff-8186-5756-a934-071f4497b58d']
   acted_on : []
   acted_using : []
   archive_uri: agave://data-sd2e-community/products/v2/106c46ff81865756a934071f4497b58d/kZgygQV2EDAAkDLRzrep1gO2/PAVpwrObxp5YjYRvrJOd5yVp

Note that the job UUID is the same now as the original demo
(``107ece67-9a94-57a1-bcb5-59b71de4fb13``), the child_of relationship resolves
to the two measurements, and the section of the archive path that incorporates
metadata linkage matches the original demo (``kZgygQV2EDAAkDLRzrep1gO2``).

Another job that processes only the third measurement, can be configured and
its archive_path will be set to the sample-based location.

.. code-block:: pycon

   >>> mpm = Job(settings.mongodb, settings.pipelines, sample_id=samples, measurement_id=measurements3)
   >>> mpm.setup()
   uuid : 107596b8-25b2-557d-9702-853f0690c576
   pipeline_uuid : 106c46ff-8186-5756-a934-071f4497b58d
   data : {}
   child_of : ['1041ab3f-5221-5c79-8781-8838dfb6eef9']
   generated_by : ['106c46ff-8186-5756-a934-071f4497b58d']
   acted_on : []
   acted_using : []
   archive_uri: agave://data-sd2e-community/products/v2/106c46ff81865756a934071f4497b58d/kZgygQV2EDAAkDLRzrep1gO2/PAVpwrObxp5YjYRvrJOd5yVp

Parameterization Data
----------------------

The contents of the ``data`` keyword argument are attached verbatim to
``PipelineJob.data``, and it is also used to establish the terminal directory
in the archive path.

.. code-block:: pycon

   >>> mpn = Job(settings.mongodb, settings.pipelines, sample_id=samples, measurement_id=measurements2, data={'alpha': 0.5})
   >>> mpn.setup()
   uuid : 107a298f-1823-582e-a936-a6b6d9bc817e
   pipeline_uuid : 106c46ff-8186-5756-a934-071f4497b58d
   data : {'alpha': 0.5}
   child_of : ['104dae4d-a677-5991-ae1c-696d2ee9884e', '10483e8d-6602-532a-8941-176ce20dd05a']
   generated_by : ['106c46ff-8186-5756-a934-071f4497b58d']
   acted_on : []
   acted_using : []
   archive_uri: agave://data-sd2e-community/products/v2/106c46ff81865756a934071f4497b58d/kZgygQV2EDAAkDLRzrep1gO2/0p5yeV3VR3OELzgoJ5kk6Yxw

The UUID is different than in **mpl** above, as is the name of the last
directory in the archive path. Thus, processing a given set of measurements
using a particular pipeline, but with different compute parameters yields a
new job with a new, but predictably defined output location.

.. code-block:: pycon

   >>> mpn = Job(settings.mongodb, settings.pipelines, sample_id=samples, measurement_id=measurements2, data={'alpha': 0.6})
   >>> mpn.setup()
   uuid : 1070b50f-6338-5a10-a8f0-943cef8ea366
   pipeline_uuid : 106c46ff-8186-5756-a934-071f4497b58d
   data : {'alpha': 0.6}
   ...
   archive_uri: agave://data-sd2e-community/products/v2/106c46ff81865756a934071f4497b58d/kZgygQV2EDAAkDLRzrep1gO2/3pGLppQE69r3Z36EY3jlxxpN

See? Varying ``alpha`` resulted in a new job and archive path.

References and Files
--------------------

The contents ``data`` are not constrained. However, if it includes the keys
``inputs`` or ``parameters``, an attempt is made to resolve those keys to
known **reference** or **file** records.

.. code-block:: pycon

   >>> mpo = Job(settings.mongodb, settings.pipelines, sample_id=samples, measurement_id=measurements2, data=data_w_inputs)
   >>> mpo.setup()
   uuid : 1079bc22-7b99-53d9-ad1c-5eeb4c191bff
   pipeline_uuid : 106c46ff-8186-5756-a934-071f4497b58d
   data : {'inputs': ['agave://data-sd2e-community/uploads/tacc/example/345.txt'], 'parameters': {'ref1': 'agave://data-sd2e-community/reference/novel_chassis/uma_refs/MG1655_WT/MG1655_WT.fa'}, 'alpha': 0.5}
   child_of : ['104dae4d-a677-5991-ae1c-696d2ee9884e', '10483e8d-6602-532a-8941-176ce20dd05a']
   generated_by : ['106c46ff-8186-5756-a934-071f4497b58d']
   acted_on : ['105fb204-530b-5915-9fd6-caf88ca9ad8a']
   acted_using : ['1099ee04-0412-5566-bb4d-0efc2af3eea3']
   archive_uri: agave://data-sd2e-community/products/v2/106c46ff81865756a934071f4497b58d/kZgygQV2EDAAkDLRzrep1gO2/RbQyWyezlxlvXOYeG81qVbG4

The **reference** asset (``MG1655_WT.fa``) is identified and associated via
``acted_using``, while the **file** asset is associated via ``acted_on``.

Interpretable inputs and parameters can be included in ``data`` by any of the
following three JSON formats. It is vastly preferable to use the URI scheme
to refer to a specific asset where possible, rather than the path-relative
form, which is provided only for edge-case compatibility with old pipelines.

.. code-block:: json
   :caption: List-style inputs

   {"inputs": [
     "/uploads/..",
     "/products/..",
     "/reference/..",
     "agave://<system>/<path>",
     "http://<external_ref>/",
     "https://<external_ref>"]
   }

.. code-block:: json
   :caption: Agave-style parameters

   {"parameters": {
       "param_name_1": "agave://<system>/<path>",
       "param_name_2: ""http://<external_ref>/",
       "param_name_3: ""https://<external_ref>/"},
       "param_name_4": "/uploads/..",
       "param_name_5": "/reference/..",
       "param_name_6": "/products/.."
   }

.. code-block:: json
   :caption: Agave-style inputs and parameters

   {"inputs": {
       "input_name_1": "agave://<system>/<path>",
       "input_name_2": "/uploads/...",
       "input_name_3": "/reference/...",
       "input_name_4": "/products/..."
    "parameters": {
       "param_name_2: ""http://<external_ref>/",
       "param_name_3: ""https://<external_ref>/"},
       "param_name_4": "/uploads/...",
       "param_name_5": "/reference/...",
       "param_name_6": "/products/..."
   }

Instanced Archive Paths
-----------------------

To assist with debugging or general collision avoidance, it is possible to
extend the normally deterministic archive path with a named/date-stamped
directory.

.. code-block:: pycon

   >>> mpp = Job(settings.mongodb, settings.pipelines, sample_id=samples, measurement_id=measurements3, instanced=True)
   >>> mpp.setup()
   uuid : 107596b8-25b2-557d-9702-853f0690c576
   pipeline_uuid : 106c46ff-8186-5756-a934-071f4497b58d
   data : {}
   child_of : ['1041ab3f-5221-5c79-8781-8838dfb6eef9']
   generated_by : ['106c46ff-8186-5756-a934-071f4497b58d']
   acted_on : []
   acted_using : []
   archive_uri: agave://data-sd2e-community/products/v2/106c46ff81865756a934071f4497b58d/kZgygQV2EDAAkDLRzrep1gO2/PAVpwrObxp5YjYRvrJOd5yVp/usable-burro-20190205T203812Z

This appends ``adjective-animal-utcZ`` as a subdirectory of archive path,
preserving the contents of the original archive path should there be any.
